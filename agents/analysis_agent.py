import spacy
from pathlib import Path
import sys

# Add the project root to Python path (si aún no está manejado globalmente)
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import the AgentFramework
from mcp.agent_framework import AgentFramework

class AnalysisAgent(AgentFramework):
    """
    Analyzes transcribed text to extract key concepts, topics,
    action items, and references.
    """
    def __init__(self):
        super().__init__("AnalysisAgent")
        self.config = self._load_config()

        # Load spaCy models (Spanish and English)
        # We'll load them as needed to save memory, or pre-load if preferred.
        self.nlp_es = None
        self.nlp_en = None
        self._load_spacy_models()

        print(f"[{self.agent_name}] Initialized.")

    def _load_spacy_models(self):
        """Loads spaCy language models."""
        print(f"[{self.agent_name}] Loading spaCy models...")
        try:
            self.nlp_es = spacy.load("es_core_news_sm")
            print(f"[{self.agent_name}] Spanish spaCy model 'es_core_news_sm' loaded.")
        except OSError:
            print(f"[{self.agent_name}] Error: Spanish spaCy model 'es_core_news_sm' not found.")
            print(f"[{self.agent_name}] Please download it: python -m spacy download es_core_news_sm")
        
        try:
            self.nlp_en = spacy.load("en_core_web_sm")
            print(f"[{self.agent_name}] English spaCy model 'en_core_web_sm' loaded.")
        except OSError:
            print(f"[{self.agent_name}] Error: English spaCy model 'en_core_web_sm' not found.")
            print(f"[{self.agent_name}] Please download it: python -m spacy download en_core_web_sm")

    def analyze_text(self, text_content: str, language: str = "es") -> dict:
        """
        Performs NLP analysis on the given text.
        
        Args:
            text_content (str): The text to analyze.
            language (str): The language of the text ('es' or 'en').
        
        Returns:
            dict: A dictionary containing extracted information.
                  (Initially, just a placeholder or basic tokenization)
        """
        nlp_model = None
        if language.lower() == "es":
            nlp_model = self.nlp_es
        elif language.lower() == "en":
            nlp_model = self.nlp_en
        
        if not nlp_model:
            print(f"[{self.agent_name}] No spaCy model loaded for language '{language}'. Cannot analyze.")
            return {"error": f"No spaCy model for language '{language}'"}

        print(f"[{self.agent_name}] Analyzing text using '{language}' model...")
        
        doc = nlp_model(text_content)
        
        # --- Initial Analysis Features (to be expanded) ---
        
        # 1. Key Concept Extraction (Noun Chunks)
        # Noun chunks are "base noun phrases" – flat phrases that have a noun as their head.
        # You can think of noun chunks as a noun plus the words describing the noun.
        key_concepts = [chunk.text for chunk in doc.noun_chunks]
        
        # 2. Named Entity Recognition (NER)
        # spaCy can identify and categorize named entities like persons, organizations, locations, etc.
        # This can be useful for tools, techniques, vulnerabilities if trained/configured.
        entities = [{"text": ent.text, "label": ent.label_} for ent in doc.ents]

        # 3. Cybersecurity Context (Simple keyword spotting for now)
        # This will be refined later with a custom dictionary/matcher
        cybersecurity_keywords = ["firewall", "malware", "phishing", "vulnerabilidad", "contraseña", "cifrado"] # Example
        mentioned_cyber_terms = []
        for token in doc:
            if token.lemma_.lower() in cybersecurity_keywords: # Use lemma for better matching
                if token.lemma_.lower() not in mentioned_cyber_terms: # Avoid duplicates
                     mentioned_cyber_terms.append(token.lemma_.lower())
        
        analysis_results = {
            "language_used": language,
            "token_count": len(doc),
            "sentence_count": len(list(doc.sents)),
            "key_concepts": list(set(key_concepts))[:15], # Top 15 unique concepts
            "named_entities": entities[:15], # Top 15 entities
            "mentioned_cybersecurity_terms": mentioned_cyber_terms
        }
        
        print(f"[{self.agent_name}] Analysis complete.")
        return analysis_results

    def run(self):
        """Main execution loop for the Analysis Agent (for manual testing)."""
        print(f"[{self.agent_name}] Running Analysis Agent in manual test mode.")

        # --- TESTING ---
        # We need a transcript file to test analysis.
        # Use one of the .txt files previously generated by the ProcessingAgent.
        transcripts_path = Path('recordings/transcripts')
        if not transcripts_path.exists():
            print(f"[{self.agent_name}] Directory not found: {transcripts_path}")
            return
        
        transcript_files = list(transcripts_path.glob('*_transcript.txt')) # Get all transcripts
        if not transcript_files:
            print(f"[{self.agent_name}] No transcript files found in {transcripts_path}.")
            print(f"[{self.agent_name}] Please run ProcessingAgent first to generate transcript files.")
            return

        # Get the most recent transcript
        transcript_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        latest_transcript_file = transcript_files[0]
        
        print(f"[{self.agent_name}] Attempting to analyze latest transcript file: {latest_transcript_file.name}")

        try:
            with open(latest_transcript_file, "r", encoding="utf-8") as f:
                # Read the main transcription text, skipping metadata lines if present
                lines = f.readlines()
                text_to_analyze = ""
                # A simple way to find the start of the actual transcript
                # Assumes "----" or "====" separates metadata from text
                for i, line in enumerate(lines):
                    if line.strip().startswith("---") or line.strip().startswith("==="):
                        if i + 1 < len(lines) and (lines[i+1].strip().startswith("---") or lines[i+1].strip().startswith("===")):
                            # Skip double separators
                            continue
                        if i + 1 < len(lines): # Check if there is text after the separator
                            text_to_analyze = "".join(lines[i+1:]) # Join all subsequent lines
                            break
                if not text_to_analyze: # Fallback if no separator found or only metadata
                     text_to_analyze = "".join(lines)


            # Determine language (this is a placeholder, a better way would be to get it from ProcessingAgent's output)
            # For now, let's assume Spanish based on your primary class language
            # or if the filename contains "_es_" or similar.
            language_to_use = "es" # Default to Spanish for this test
            if "auto" in latest_transcript_file.name.lower():
                # If it's an auto-detected transcript, we might try to guess or use the detected lang
                # For now, we'll stick to 'es' for simplicity in testing the Spanish model.
                # Or, you could try to parse the "Detected Language: xx" line from the transcript file.
                pass 
            
            print(f"[{self.agent_name}] Using language: '{language_to_use}' for analysis.")
            results = self.analyze_text(text_to_analyze.strip(), language=language_to_use)
            
            if results and "error" not in results:
                print(f"[{self.agent_name}] Test analysis successful for {latest_transcript_file.name}:")
                for key, value in results.items():
                    print(f"  {key.replace('_', ' ').title()}: {value}")
            elif results:
                print(f"[{self.agent_name}] Test analysis encountered an error: {results.get('error')}")
            else:
                print(f"[{self.agent_name}] Test analysis failed or returned no results.")

        except Exception as e:
            print(f"[{self.agent_name}] Error during test run: {e}")


if __name__ == "__main__":
    # Ensure the virtual environment is sourced before running this directly
    # python agents/analysis_agent.py
    
    analysis_agent = AnalysisAgent()
    analysis_agent.run()
